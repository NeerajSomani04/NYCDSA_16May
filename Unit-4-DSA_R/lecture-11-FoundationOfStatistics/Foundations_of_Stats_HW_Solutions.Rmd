---
title: "Foundations_of_Stats_Homework_Solutions"
author: "NYC Data Science Academy"
output: html_document
---
## Question #1: Body Temperature

The `Temp.txt` dataset records the body temperatures, gender, and heart rate of 130 individuals; you may assume that the observations are independent of one another.

1. Read the data into R and provide basic numerical EDA to describe the data.
  + Provide univariate EDA for all variables, and bivariate EDA where appropriate.

2. Provide basic graphical EDA to describe the data.
  + Provide univariate EDA for all continuous variables, and bivariate EDA where appropriate.

3. You have heard that the average human body temperature is 98.6 degrees Fahrenheit. Does this data support this claim? (Perform a hypothesis test to determine whether the true population mean body temperature is 98.6 degrees Fahrenheit.)
  + What is a 95% confidence interval for the average human body temperature?
  + Interpret your results in context of the problem.

4. Is there a significant difference in body temperature between males and females? If so, quantify this difference.
  + What is a 95% confidence interval for the average difference in human body temperature between males and females?
  + Interpret your results in context of the problem.
  + [**Bonus**] If we had collected a lot of samples of the body temperature for different genders, we could have directly compared *the average of the average temperature* for each gender. Hypothesis tests we have covered allow us to deduce the distribution of the average without needing to collect many samples (we have only 65 males and 65 females). Another simple way to achieve this is to **bootstrap**. Random sample (with replacement) 65 observations from each gender, then use them as a new sample. By bootstrapping, derive the 95% confidence interval for the average difference in human body temperature between males and females.

5. You believe the variances of male heart rate and female heart rate are different; specifically, you claim that females have a lower heart rate variance. Test this claim.
  + What is a 95% confidence interval for the ratio between female and male heart rate variances?
  + Interpret your results in context of the problem.

```{r, warning = FALSE}

temp = read.table('https://s3.amazonaws.com/graderdata/Temp.txt', header = TRUE)

#1a Example numerical EDA.
summary(temp)
# We can also use str(),dim(),nrow(),ncol() to know the dataset
sd(temp$Body.Temp)
sd(temp$Heart.Rate)
# use sapply: sapply(temp, sd) 
cor(temp$Body.Temp, temp$Heart.Rate) # Bivariate methods
#http://purpledreams.com/~mike/math_studies_11/statistics_08.pdf
# no strong linear relationship

#2a Example graphical EDA.
# check the distributions
hist(temp$Body.Temp, prob = T) 
# what if we change prob = T? plot by frequency; we want to plot probability density here
lines(density(temp$Body.Temp), col = "red")

hist(temp$Heart.Rate, prob = TRUE)
lines(density(temp$Heart.Rate), col = "red")

# bivariate density plot
plot(density(temp$Body.Temp[temp$Gender == "Female"]), col = "pink")
lines(density(temp$Body.Temp[temp$Gender == "Male"]), col = "blue")

plot(density(temp$Heart.Rate[temp$Gender == "Male"]), col = "blue")
lines(density(temp$Heart.Rate[temp$Gender == "Female"]), col = "pink")

# scatter plot
plot(temp$Body.Temp, temp$Heart.Rate, col = temp$Gender)

#3ab One Sample T-Test
# H0: Aveage(body_temperature)=96.8
t.test(temp$Body.Temp, mu = 98.6, alternative = "two.sided")

#The p-value for this test is extremely small (<.0005); thus, we reject the null
#hypothesis that the average human body temperature is 98.6 degrees F in favor of
#the alternative that the average temperature is different than 98.6 degrees F at the
#95% confidence level. 

#We are 95% confident that the true average human body
#temperature is between 98.122 degrees F and 98.377 degrees F.


#4ab Two Sample T-Test
# H0: Aveage(male body_temperature)=Aveage(female body_temperature)

t.test(temp$Body.Temp[temp$Gender == "Female"],
       temp$Body.Temp[temp$Gender == "Male"],
       alternative = "two.sided")
t.test(temp$Body.Temp ~ temp$Gender, alternative = "two.sided") #Equivalent.

#The p-value for this test is about 0.024, which is less than our cutoff value
#of 0.05; thus, we reject the null hypothesis that the average human body
#temperature is the same for males and females in favor of the alternative that
#they differ in some manner at the 95% confidence level. We are 95% confident
#that the true difference is between 0.039 and 0.540 degrees F, with Females
#having a higher average body temperature.

#4c (BONUS)
library(dplyr)
Ma = temp %>% filter(Gender == 'Male')
Fe = temp %>% filter(Gender == 'Female')

avg = numeric()
for(i in 1:10000){
  m = sample(Ma$Body.Temp, 65, replace=T)
  f = sample(Fe$Body.Temp, 65, replace=T)
  avg[i] = mean(f-m)
}
lower_bnd = mean(avg) - 2*sd(avg)
upper_bnd = mean(avg) + 2*sd(avg)

#5ab F-Test
#H0: Average(male body_temperature)=Average(female body_temperature)
#H1: Average(male body_temperature)<Average(female body_temperature)
# http://2012books.lardbucket.org/books/beginning-statistics/s15-chi-square-tests-and-f-tests.html
var.test(temp$Heart.Rate[temp$Gender == "Female"],
         temp$Heart.Rate[temp$Gender == "Male"],
         alternative = "less")

#Under the null hypothesis, males and females have the same heart rate variance;
#we test whether females have a statistically significantly lower heart rate
#variance than males. We find that the p-value is extremely high (0.9945). Thus,
#we retain our null hypothesis. We are 95% confident that the true ratio is
#between 0 and 2.88 -- note that this interval contains 1 (the null hypothesis).

#Change the H1:
#H0: Average(male body_temperature)=Average(female body_temperature)
#H1: Average(male body_temperature)>Average(female body_temperature)
var.test(temp$Heart.Rate[temp$Gender == "Female"],
         temp$Heart.Rate[temp$Gender == "Male"],
         alternative = "greater") #p-value = 0.005502

# 95 percent confidence interval:
#   1.257668      Inf 
# Why Inf?
# When should we use alternative = "two.sided"

```

## Question #2: Plant Growth

Load the `PlantGrowth` dataset located in the `datasets` library; this dataset contains dried plant weight measurements for the same species of plant under three different conditions (two separate growth treatments, and a control group where no treatment was applied). You may assume that the observations are independent of one another.

1. Create side-by-side boxplots of the plant weights segmented by the type of applied treatment. Describe the features of the graph.

2. Calculate the standard deviations of each conditional distribution of plant weight based on the applied treatment. Do these differ significantly?
  + **NB**: To avoid increasing our chance of encountering a "false positive," we must avoid applying three separate F-tests (treatment #1 vs treatment #2; treatment #1 vs control; treatment #2 vs control). As an alternative, Bartlett's Test of Homogeneity of Variances allows us to simultaneously test for the similarity of a group of variances, rather than just a pair. Implement this test using the `bartlett.test()` function and report your results.
  
3. Is there a significant difference in the weight of plants based on the growth treatments they were given? Conduct a hypothesis test and report your results in context of the problem. 
  + Given the results of the Bartlett test, is the result of your hypothesis test valid?



```{r}
library(ggplot2)
library(datasets)
PlantGrowth

#1 Creating side-by-side boxplots of the data.
boxplot(weight ~ group, data = PlantGrowth)

#2a Calculating the standard deviations of each treatment group.
sd(PlantGrowth$weight[PlantGrowth$group == "ctrl"])
sd(PlantGrowth$weight[PlantGrowth$group == "trt1"])
sd(PlantGrowth$weight[PlantGrowth$group == "trt2"])

# Use ggplot 
ggplot(PlantGrowth, aes(x = group, y = weight))+
  geom_boxplot(color = c("red", "blue", "green"))


#Conducting the Bartlett test of homogeneity of variances.
bartlett.test(PlantGrowth$weight ~ PlantGrowth$group)

#The p-value for this test is > 0.05; we do not have statistical evidence to
#conclude that the variances of the plant treatment groups are different.

#3a One-Way ANOVA - access the equality of means of two or more groups?
# What the relationship between a Two sample T-test and One-way ANOVA,if there are exactly two groups?
summary(aov(weight ~ group, data = PlantGrowth))
# plot(aov(weight ~ group, data = PlantGrowth))


#The p-value for this test is 0.02, which is smaller than our cutoff value of
#0.05; we have evidence to conclude that the average plant weight varies by the
#type of treatment that was applied. This conclusion is valid based on the
#Bartlett test because we determined that the group variances are not statistically
#different from one another.

#3b multiple comparisons 
# https://en.wikipedia.org/wiki/Multiple_comparisons_problem

```

## Question #3: Gender, Hair, & Eye Color

Load the `HairEyeColor` dataset located in the `datasets` library; this is a three dimensional dataset that records the hair color, eye color, and gender of 592 different statistics students.

1. Visualize the entire dataset using a mosaic plot using the following command:
  + `mosaicplot(HairEyeColor, shade = TRUE)`
  + Which category combinations receive more observations than expected? Fewer observations than expected?
  
2. Reduce your dataset to focus on just females with brown and blue eyes (but still include all hair colors). Create another mosaic plot and describe what you see.
  + Conduct a hypothesis test to see if hair and eye color are independent of one another for this reduced dataset. Report your results in context of the problem.

3. For the reduced dataset, which category combination contributed most to any statistical deviations? Which category contributed the least? By how much for each?


```{r}

str(HairEyeColor)
#1ab Visualizing the data with a mosaic plot.
# A mosaic plot is a graphical display that allows you to 
# examine the relationship among two or more categorical variables.
mosaicplot(HairEyeColor, shade = TRUE)
#https://medschool.vanderbilt.edu/cqs/files/cqs/media/DrTsai2_0.pdf

#Whereas most categories do not deviate very far from expected combination values,
#some appear to have high standardized residuals. In particular, the blond hair
#category for both genders with brown eyes does not contain a lot of observations.
#On the other hand, the blond hair category has a lot more than expected females
#with blue eyes.

#2a Reducing the dataset to all hair colors, brown & blue eye colors, and just
#females.
reduced = HairEyeColor[,1:2,2]

#Visualizing the reduced data with a mosaic plot.
mosaicplot(HairEyeColor[,1:2,2], shade = TRUE)

#Chi-Squared test of independence.
reduced.test = chisq.test(reduced)
reduced.test

#The chi-squared test reports a significant p-value at the 0.05 level; thus we
#conclude that the hair and eye colors of females are not independent of one
#another.

#3 Calculating each category combination's contribution to the chi-squared
#statistic.
# H0: The hair and eye colors are independent of one another

(reduced.test$observed - reduced.test$expected)^2/reduced.test$expected
reduced.test$observed - reduced.test$expected
# reduced.test$expected:the expected counts under the null hypothesis.

#Blonde hair and blue eyed females contributed the largest proportion to the chi-
#squared statistic with a value of 29.55; there were about 31 more observations
#in this category than we would have expected under the assumption of independence.
#Red hair and brown eyed females contributed the least proportion to the chi-
#squared statistic with a value of 1.42; there were only about 4 more observations
#in this category than we would have expected under the assumption of independence.
```

