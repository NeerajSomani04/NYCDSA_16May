{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications_with_NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** A general introduction to NLP applications to serve as a foundation for further self study and additional NLP curriculums. This notebook intends to serve as a basis for various techniques such as vectorizing text, classifying text and topic modeling.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "Open your terminal/Anaconda Prompot, cd to the lecture code folder and run the following commands:\n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have examined methods for preprocessing text in the previous NLP Jump Start, let's examine methods of converting text to vectors of the modeling stage. Within NLP, there are two predominant methods: **frequency based** and **prediction based**.\n",
    "\n",
    "- When you hear of \"bag-of-words\" or \"TF-IDF\", it is referring to frequency based while \"word2vec\" or \"doc2vec\" refer to prediction based. \n",
    "\n",
    "- Note, **NEITHER** of these embeddings are your actual classifiers/regressors but instead used to understand text behavior or pass as the inputs into a predictive model.\n",
    "\n",
    "- For purposes of this tutorial, we will focus on the **frequency based** methods as the others often uses pre-trained neural network models and as a result, has less use for understanding of foundational NLP methods. \n",
    "\n",
    "- Since frequency based methods lean traditionally on the bag-of-words method, it doesn't capture positions or semantics.  However, despite this, the output vectors of these models can perform well in a variety of NLP predictive problems. (ex. a high frequency of \"free\", despite various positional text, generally would indicate an email is spam in the classic example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Often this method is often referred to as \"bag-of-words\" but CountVectorizer can includes methods of n-grams to handle more complex tasks. At it's basic principle, it's simply a count of words (or permutation of words) throughout the document.\n",
    "\n",
    "- In this scheme, features and samples are defined as follows:\n",
    " - each **individual token occurrence frequency** is treated as a **feature**.\n",
    " - the vector of all the token frequencies for a given **document** is considered a **multivariate sample**.\n",
    "- A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The data**: 20 newsgroups dataset that we will use today comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note `newsgroups_train.data` will be a list of strings. The same method and code structure will work if you pass the column of text from your pandas dataframe as well. Let's walk through how to transform the data through CountVectorization and TF-IDF respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = newsgroups_train.target[0]\n",
    "print(newsgroups_train.target_names[category_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "X_train_count = count_vec.fit_transform(newsgroups_train.data)\n",
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "- In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the `scipy.sparse` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(X_train_count.todense()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print out the token in the corpus every 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vec.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to count not just **uni-gram** but **bigram** as well, we can set the `ngram_range=(1, 2)`. \n",
    "- Check the size of the output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_count = count_vec.fit_transform(newsgroups_train.data)\n",
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print out the unigram and bigrams every 5000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vec.get_feature_names()[::5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on the size of your corpus, you can also set the `min_df` parameter \n",
    " - ignore terms that have a document frequency lower than the given threshold.\n",
    " - the size of the output matrix is much smaller than the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(ngram_range=(1,2), min_df=10)\n",
    "X_train_count = count_vec.fit_transform(newsgroups_train.data)\n",
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(count_vec.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **Term Frequency** is a count of how many times a word occurs in a given document (synonymous with bag of words). The **Inverse Document Frequency** is the the number of times a word occurs in a corpus of documents. TF-IDF goes one step further. It applies a frequency count but then penalizes it by dividing it across the appearance throughout all documents.\n",
    "\n",
    "- One possible definition of TF-IDF is:\n",
    "\n",
    "$$tf_{t,d} = log(1+f_{t,d})$$\n",
    "\n",
    "$$idf_{t,d} = log(1+\\frac{N}{df_{t}})$$\n",
    "\n",
    "$$w_{t,d} = tf_{t,d}\\times idf_{t,d}$$\n",
    "\n",
    "where N is the number of documents in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why is log used when calculating term frequency and inverse document frequency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If term frequency for word 'AI' in doc1 is 10 and doc2 is 20, we can say that doc2 is more relevant than doc1 for word 'AI'. However, if the term frequency of the same word, 'AI' for doc1 is 1 million and doc2 is 2 million, at this point, there is no much different in term of relevant anymore because they both contain a very high count for term 'AI'.\n",
    "\n",
    "- Adding log is to **dampen** the importance of term that has a high frequency, e.g. Using log base 2, the count of 1 million will be reduced to 19.9! \n",
    "\n",
    "- We also add 1 to the log(tf) because when tf is equal to 1, the log(1) is zero. by adding one, we distinguish between tf=0 and tf=1.\n",
    "- The `TfidfVectorizer` is equivalent to `CountVectorizer` followed by `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "X_train_tf = tf_idf.fit_transform(newsgroups_train.data)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we've examined how to prepare text, transform text, we can examine various methods of predicting with our matrix. The traditional model for text predictions is Naive Bayes.\n",
    "\n",
    "- Naive Bayes assumes that terms within documents are independent of each other. The classic example of this is the email spam detection. Consider implementation of this model on small amounts of texts across each document (ie reviews for negative, positive.) It’s important to note, because of the assumption that the terms would be independent of each other, removing highly correlated features before running the model would improve performance greatly.\n",
    "\n",
    "- There are two prevailing models: multivariate bernoulli model vs multinomial model:\n",
    " - The Multivariate Bernoulli model which ignores the frequency of words and Multinomial model which takes the frequency into account. \n",
    " - Since we are implementing frequency rather than Boolean, it should be apparent that we will be using the Multinomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to predict the category of each news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's do a mock example of checking what are predicted tags using both sets of inputs.\n",
    "\n",
    "- First we will create and train the model on the two training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiNB = MultinomialNB()\n",
    "\n",
    "cntvecMNB = multiNB.fit(X_train_count, newsgroups_train.target)\n",
    "tf_idfMNB = multiNB.fit(X_train_tf, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the models are trained, we will send new statements using the same CountVectorizer and TF-IDF modeled we trained earlier to transform our new statements as new test statements and predict those new vectors with our Multinomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_docs = [\"\"\"In the ancient and medieval world  \n",
    "            the etymological Latin root religio was understood as an individual virtue of worship \n",
    "            never as doctrine, practice, or actual source of knowledge.  \n",
    "            Furthermore, religio referred to broad social obligations to family, neighbors, rulers, and even towards God. \n",
    "            When religio came into English around the 1200s as religion, it took the meaning of \"life bound by monastic vows\". \n",
    "            The compartmentalized concept of religion, where religious things were separated from worldly things, \n",
    "            was not used before the 1500s. The concept of religion was first used in the 1500s to distinguish \n",
    "            the domain of the church and the domain of civil authorities.\"\"\",\n",
    "            \n",
    "           \"\"\"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and \n",
    "           alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. \n",
    "           GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. \n",
    "           Modern GPUs are very efficient at manipulating computer graphics and image processing, \n",
    "           and their highly parallel structure makes them more efficient than general-purpose CPUs  \n",
    "           for algorithms where the processing of large blocks of data is done in parallel. \n",
    "           In a personal computer, a GPU can be present on a video card, or it can be embedded \n",
    "           on the motherboard or—in certain CPUs—on the CPU die\"\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Important**: We do **NOT** fit the test data through a new CountVectorizer model or a new TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc_count = count_vec.transform(new_docs)\n",
    "new_doc_tfidf = tf_idf.transform(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_predicted = cntvecMNB.predict(new_doc_count)\n",
    "tfidf_predicted = tf_idfMNB.predict(new_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in cnt_predicted:\n",
    "    print(newsgroups_train.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tfidf_predicted:\n",
    "    print(newsgroups_train.target_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So here we see both predictions accurately label the two but let's evaluate the behavior from an accuracy metrics of the two word embedding inputs.\n",
    "- Try to see if you can walk through the steps you need to do before reading the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_count = count_vec.transform(newsgroups_test.data)\n",
    "X_test_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_tf = tf_idf.transform(newsgroups_test.data)\n",
    "X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec_predicted = cntvecMNB.predict(X_test_count)\n",
    "tfidf_predicted = tf_idfMNB.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can generate a classification report to examine how our models perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('The report for CountVectorizer word embedding through a Multinomial model:')\n",
    "print(metrics.classification_report(newsgroups_test.target, countvec_predicted, target_names= newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The report for TF-IDF Vectorizer word embedding through a Multinomial model:')\n",
    "print(metrics.classification_report(newsgroups_test.target, tfidf_predicted, target_names= newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that TF-IDF performs significantly better in recall. Try to examine why this may be the case now knowing what you learned about the difference between CountVectorizer and TF-IDF.\n",
    "- Consider instances where CountVectorizer may work better than TF-IDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "- [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) or LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "- Each document is modeled as a multinomial distribution of **topics** and each topic is modeled as a multinomial distribution of **words**.\n",
    "\n",
    "- It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/lda_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above chart shows how LDA tries to classify documents. Documents are represented as a distribution of topics. \n",
    "\n",
    "- Topics, in turn, are represented by a distribution of all tokens in the vocabulary. But we do not know the number of topics that are present in the corpus and the documents that belong to each topic. In other words, we want to treat the assignment of the documents to topics as a random variable itself which is estimated from the data.\n",
    "\n",
    "- This sounds complicated, but the process we have discussed above is similar to the Dirichlet process, which we will dig into later after we see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemtzer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return lemtzer.lemmatize(text, pos='v')\n",
    "\n",
    "# Write a function to perform the pre processing steps on the entire dataset\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines if this is the first time you use nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preview a document after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's now preprocess all the news headlines we have. To do that, we iterate over the list of documents in our training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_docs  = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's create a dictionary from `processed_docs` containing the index of each word appears in the training set. To do that, let's pass `processed_docs` to `gensim.corpora.Dictionary()` and call it `dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter out tokens that appear in\n",
    " - less than `no_below` documents (absolute number) or\n",
    " - more than `no_above` documents (fraction of total corpus size, not absolute number).\n",
    " - after (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 20% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.2, keep_n=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some of the parameters we will be tweaking are:\n",
    " - `num_topics` is the number of requested latent topics to be extracted from the training corpus.\n",
    " - `id2word` is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    " - `passes` is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:\n",
    "- We choose the number of topics to be 6 in our lda model as partitioned according to subject matter on the [website](http://qwone.com/~jason/20Newsgroups/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2)\n",
    "lda_model.save('lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each topic, we can explore the words occuring in that topic and its relative weight.\n",
    "- Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main use of the pyLDAvis package to provide interactive visualizations to augment our understanding. Let us see how our topics look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can understand the document from the following perspective.\n",
    " - Topics are represented as a bubble. The size of the bubble is proportional to its prevalence of the corpus.\n",
    " - Similar topics appear close together, topics further apart are less similar.\n",
    "- Upon selecting a topic, the most representative words for the selected topic can be seen. This measure can be a combination of how frequent or how discriminant the word is. You can adjust the weight of each property using the slider.\n",
    "- When a topic is selected, the percentage of tokens in the topic is also visible. This measure can be used as an additional measure to weed out irrelevant topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct the tfidf corpus from our bag-of-words corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_corpus, num_topics=6, id2word=dictionary, passes=2)\n",
    "lda_model_tfidf.save('lda_tfidf.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dig into the details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is Dirichlet distribution?\n",
    "- The idea of the Dirichlet process is simple; we assign elements to categories following a very simple rule: When assigning the nth element, we assign it to a new category with the probability\n",
    "\n",
    "$$\\frac{\\alpha}{\\alpha + n -1}$$\n",
    "\n",
    "or we assign it to an already existing category x with probability\n",
    "\n",
    "$$\\frac{n_x}{\\alpha + n -1}$$\n",
    "\n",
    "where $n_x$ is the number of random variables already assigned to category $X$. What’s $\\alpha$?\n",
    "\n",
    "- In case of the Dirichlet distribution, it is a conjugate prior for the multinomial distribution. \n",
    " - If in the case of the binomial distribution we can think of it in terms of drawing white and black balls with replacement from the urn, then in case of the multinomial distribution we are drawing with replacement N balls appearing in k colors, where each of colors of the balls can be drawn with probabilities $p_1,...,p_k$. \n",
    "  - The Dirichlet distribution is a conjugate prior for $p_1,...,p_k$ probabilities and $\\alpha_{1},...,\\alpha_{k}$ parameters can be thought as pseudocounts of balls of each color assumed a priori.\n",
    " - The higher value of $\\alpha$, the greater amount of the total \"mass\" is assigned to it. If $\\alpha < 1$, it can be thought as anti-weight that pushes away each point toward extremes, while when it is high, it attracts each point toward some central value (central in the sense that all points are concentrated around it, not in the sense that it is symmetrically central)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://phyletica.org/images/dpp-3-example.gif)\n",
    "[Souce](http://phyletica.org/dirichlet-process/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LDA in graphical form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diagram what we see here is called the **plate notation for LDA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/lda_plate.png)\n",
    "[Source](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- M is the superset of all the documents\n",
    "- N is the number of words per document\n",
    "- Inside the rectangle N we see w and z which can thought of as\n",
    " - w -> the words observed in document i\n",
    " - z -> the random topic for jth word for document i\n",
    "- theta -> Topic distribution for document i\n",
    "- alpha -> Parameter to set prior Dirichlet distribution to per document level\n",
    "- beta -> Parameter to set prior Dirichlet distribution at per topic word level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Key things to remember when optimizing on alpha and beta**\n",
    "\n",
    "- **High Alpha** indicates that each document is more likely to have a mixture of all the topics\n",
    "- **Low Alpha** indicates that each document is more likely to have a mixture of one/two or few of the topics\n",
    "- **High Beta** indicates that each topic is more likely to have a mixture of all the words\n",
    "- **Low Beta** indicates that each topic is more likely to have a mixture of one/two or few of the words\n",
    "\n",
    "- Note: these two parameters are called **alpha** and **eta** respectively in the gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
