{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Boosting Homework\n",
    "- In this problem set, we study the **wage** dataset\n",
    "- Because **gbm** is a tree based model, we do not need to dummify all the nominal categorical variables\n",
    "- Instead we only need to relabel them using **sklearn**'s **LabelEncoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wage = pd.read_csv('Wage.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'age', 'maritl', 'race', 'education', 'region', 'jobclass',\n",
       "       'health', 'health_ins', 'logwage', 'wage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Reset the index of wage dataframe\n",
    "\n",
    "wage.index = range(wage.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Part A\n",
    "- Explain in plain English why dummification is often not necessary for tree-based models\n",
    "- Apply the **fit_transform** method of sklearn **LabelEncoder** to each of the nominal categorical columns and convert their string values to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1 In Plain English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lencoder = LabelEncoder()\n",
    "## make sure that you realize that the labelEncoder accepts numpy array\n",
    "race_labels      =lencoder.fit_transform().reshape((-1,1))\n",
    "education_labels = \n",
    "maritl_labels    = \n",
    "health_labels    = \n",
    "health_ins_labels= \n",
    "\n",
    "age_labels       = \n",
    "year_labels      = \n",
    "logwage_labels   = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Combine the **race_labels**, **education_labels**, **maritl_labels**, **health**, \n",
    "**health_ins**, **age**, **year**, **logwage** columns to generate a new feature numpy array (or pandas dataframe)\n",
    "(hint: np.concatenate function or pd.concat can be useful) called **wageFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wageFeatures = ?\n",
    "columnNames  = ['logwage', 'age', 'year', 'race', 'education', 'maritl', 'health', 'health_ins']\n",
    "wageFeatures = pd.DataFrame(?, columns = ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Q1 Part B\n",
    "- Fit a **GradientBoostingClassifier** model (with the default setting) of jobclass against the newly generated wageFeatures and report \n",
    "      - accuracy\n",
    "      - feature_importance\n",
    "      and sort the features by their importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The accuracy is %.3f' %())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(wageFeatures.columns, ), key=, reverse=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Part C: The Effect of Hyper-parameter n_estimators vs Learning_rate\n",
    "\n",
    "- In this question, we study how does tuning the **n_estimators** hyperparameters affect the accuracy, making use of the\n",
    "**staged_predict** method of the **gbm** object\n",
    "- Recall that **staged_predict** outputs a python generator and the generator outputs the prediction iteratively using next()\n",
    "- Setting **n_estimators** to $10000$ and fit a model for **learning_rate=1**, **learning_rate=0.1** (default) and **learning_rate=0.01**, respectively\n",
    "- Running for loops to extract the staged_predictions and record the accuracies at range(100, 10100, 100)\n",
    "- Plot the accuracy curves for **learning_rate=0.1**/**learning_rate=0.01** and compare them\n",
    "- Summary your observation in plain English\n",
    "\n",
    "${\\bf Hint}:$ To compute the accuracy between the true labels and the predicted label, one uses the \n",
    "    **sklearn.metrics.accuracy_score** function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_estimators = 10100\n",
    "gbm.set_params(n_estimators=n_estimators)\n",
    "steps = list(range(100,10100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm.set_params(learning_rate = ?)\n",
    "gbm.fit(?,?)\n",
    "gen = gbm.staged_predict(?)\n",
    "scores_rate1 = []\n",
    "for n in range(n_estimators):\n",
    "                predicted_labels = next(gen)\n",
    "                if n not in steps: continue\n",
    "                scores_rate1.append(?)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm.set_params(learning_rate = ?)\n",
    "gbm.fit(?,?)\n",
    "gen = gbm.staged_predict(?)\n",
    "scores_rate01 = []\n",
    "for n in range(n_estimators):\n",
    "                predicted_labels = next(gen)\n",
    "                if n not in steps: continue\n",
    "                scores_rate01.append(?)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm.set_params(learning_rate = ?)\n",
    "gbm.fit(?,?)\n",
    "gen = gbm.staged_predict(?)\n",
    "scores_rate001 = []\n",
    "for n in range(n_estimators):\n",
    "                predicted_labels = next(gen)\n",
    "                if n not in steps: continue\n",
    "                scores_rate001.append(?)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting The Accuracy curves vs the Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(?, ?, label='learning rate = 1')\n",
    "plt.plot(?, ?, label='learning rate = 0.1')\n",
    "plt.plot(?, ?, label='learning rate = 0.01')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is clear from the above picture that dropping **learning_rate** slows down the rate the **gbm** achieves the desirable performance\n",
    "- The best performance in the above plot (100% accuracy) is likely due to overfitting, which can be verified only after train-test split or gridsearch + cross validation\n",
    "- The **learning_rate=1** curve reaches $100\\%$ accuracy before $1000$ trees. The **learning_rate=0.01** curve reaches\n",
    "only $82\\%$ accuracy at about $10000$ trees. The **learning_rate=0.1** curve reaches $100\\%$ accuracy near $10000$ trees, but \n",
    "the accuracy climbs slowly in-between. Unlike the **learning_rate=1** curve which tends to overshoot quickly, among these three **learning_rate=0.1** seems to strike a balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Part D:\n",
    "- With **learning_rate=0.01**, compare the accuracy curves (with **n_estimators=10100** as above) of \n",
    "**subsample = 1.0**, **subsample = 0.9** and **subsample=0.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm.set_params(learning_rate = ?, subsample = ?)\n",
    "gbm.fit(?,?)\n",
    "gen = gbm.staged_predict(?)\n",
    "scores_subsample09 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_labels = next(gen)\n",
    "           if n not in steps: continue\n",
    "           scores_subsample09.append(?)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm.set_params(learning_rate = ?, subsample = ?)\n",
    "gbm.fit(?,?)\n",
    "gen = gbm.staged_predict(?)\n",
    "scores_subsample01 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_labels = next(gen)\n",
    "           if n not in steps: continue\n",
    "           scores_subsample01.append(?)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(?, ?, label = 'subsample=1.0')  # not taking subsample\n",
    "plt.plot(?, ?, label = 'subsample=0.9')\n",
    "plt.plot(?, ?, label = 'subsample=0.1')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary on the Subsampling\n",
    "- Subsampling is the **gbm**'s analogue of tree-bagging where each tree estimates only a random subset of the full samples\n",
    "- The **subsample** controls the percentage of samples used to fit each tree\n",
    "- From the above analysis it is clear that by dropping **subsample=1** to **subsample=0.9** improves the performance, especially\n",
    "beyond $3000$ trees\n",
    "- On the other hand, the **subsample=0.1** reduces the accuracies significantly\n",
    "- Similar to the idea of **random forest**, adding subsampling to the **gbm** model improves its performance. But overusing it\n",
    "could have back-fired, reducing the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Part A GBM Regressor\n",
    "- In this exercise, we train a **gbm** model for a nonlinear regression task\n",
    "- We use **age**, **year**, **race_label**, **education_label**, **maritl_label**, **health_label**, **health_ins_label** to generate a new numpy feature array (or pandas dataframe) called wageFeatures2\n",
    "- Then we run a **GraidentBoostingRegressor** on **logwage** against wageFeatures2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbmr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wageFeatures2 = np.concatenate()\n",
    "wageFeatures2 = pd.DataFrame(wageFeatures2, columns=['age','year','race','education','maritl','health','health_ins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a warm up, run a **gbm** regressor model (with default setting) on (wageFeatures2, wage.logwage) and determine its $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmr.fit(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The R^2 is %.3f' %())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Part B: Use Staged_Predict to Generate the Intermediate Predictions\n",
    "\n",
    "- Parallel to the classification task, use **staged_predict** to study the improvement of $R^2$ against the number of trees\n",
    "- Set the **n_estimators = 50100** \n",
    "- Use **staged_predict** to generate a new python generator\n",
    "- Compute the $R^2$ at steps = range(100, 50100, 1000)\n",
    "- Plot the $R^2$ curves for **learning_rate = 1, 0.1, 0.01** and compare their results. Summarize your finding\n",
    "- Use **sklearn.metrics.r2_score** to compute the $R^2$ between the true targets and the predicted targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "n_estimators = 50100\n",
    "steps = range(100, 50100, 1000)\n",
    "\n",
    "gbmr.set_params(learning_rate = 1, n_estimators=n_estimators, max_depth=3)\n",
    "gbmr.fit(, )\n",
    "gen = gbmr.staged_predict()\n",
    "r2_rate1 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_targets = next(gen)\n",
    "           if n not in steps: continue\n",
    "           r2_rate1.append()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmr.set_params(learning_rate = 0.1)\n",
    "gbmr.fit(, )\n",
    "gen = gbmr.staged_predict()\n",
    "r2_rate01 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_targets = next(gen)\n",
    "           if n not in steps: continue\n",
    "           r2_rate01.append()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmr.set_params(learning_rate = 0.01)\n",
    "gbmr.fit(,)\n",
    "gen = gbmr.staged_predict()\n",
    "r2_rate001 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_targets = next(gen)\n",
    "           if n not in steps: continue\n",
    "           r2_rate001.append()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(, ,  label=r'R^2 curve for learning_rate = 1')\n",
    "plt.plot(, , label=r'R^2 curve for learning_rate = 0.1')\n",
    "plt.plot(, , label=r'R^2 curve for learning_rate = 0.01')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary on $R^2$ Curves\n",
    "- The above plot illustrates that dropping **learning_rate** has a negative effect on $R^2$ curves. \n",
    "Overall the performance is reduced, and it takes more iterations to achieve the same $R^2$ scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Part C: max_depth\n",
    "- **GradientBoostingRegressor** has a attribute called **max_depth**, which controls the heights of individual trees. \n",
    "Compute the $R^2$ curve with **learning_rate=0.1** and **max_depth=5, 10** and compare with\n",
    "the above result with the previous result **learning_rate=0.1**, **max_depth = 3** (default)\n",
    "- Plot all of these $R^2$ curves and compare. Summarize your finding in plain English\n",
    "- Use a 70%-30% train-test split on the **learning_rate=0.1, max_depth=5** model to find out the **n_estimators** where over-fitting starts to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmr.set_params(learning_rate = 0.1, max_depth = )\n",
    "    \n",
    "gbmr.fit(, )\n",
    "gen = gbmr.staged_predict()\n",
    "r2_maxdepth5 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_targets = next(gen)\n",
    "           if n not in steps: continue\n",
    "           r2_maxdepth5.append()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmr.set_params(learning_rate = 0.1, max_depth = )\n",
    "    \n",
    "gbmr.fit(,)\n",
    "gen = gbmr.staged_predict()\n",
    "r2_maxdepth10 = []\n",
    "for n in range(n_estimators):\n",
    "           predicted_targets = next(gen)\n",
    "           if n not in steps: continue\n",
    "           r2_maxdepth10.append()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(, , label='max_depth = 3')\n",
    "plt.plot(, , label='max_depth = 5')\n",
    "plt.plot(, , label='max_depth = 10')\n",
    "plt.title('$R^2$ curves with different depths')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Individual trees are well known to be the so-called **weak learners**. Increasing the **max_depth** \n",
    "increases the complexity of the trees.   On the surface the $R^2$ score improves with the same number of trees\n",
    "- The **max_depth=10** $R^2$ curve plateaus at $R^2\\sim 0.9$, indicating it could be an overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(, , test_size=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estimators = 50100\n",
    "gbmr.set_params(learning_rate=0.1, max_depth=5, n_estimators=n_estimators)\n",
    "train_r2 = []\n",
    "test_r2  = []\n",
    "steps = range(100,50100,1000)\n",
    "\n",
    "gbmr.fit(, )\n",
    "gen_train = gbmr.staged_predict()\n",
    "gen_test  = gbmr.staged_predict()\n",
    "\n",
    "for n in range(n_estimators):\n",
    "           predicted_train = next(gen_train)\n",
    "           predicted_test  = next(gen_test)\n",
    "           if n not in steps: continue\n",
    "           train_r2.append()\n",
    "           test_r2.append()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(steps, train_r2, label='train set $R^2$')\n",
    "plt.plot(steps, test_r2, label='test set $R^2$')\n",
    "plt.legend(loc=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "- It is apparently overfitting throughout the whole range, where we have sampled $R^2$ with 1000-tree increments \n",
    "- Unlike **multiple linear regression** where $R^2$ is guaranteed to be positive, the large tree limit of the **gbm** regressor produces negative $R^2$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
