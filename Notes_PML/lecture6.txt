Discriminant Analysis and Naive Bayes  -->

1. Conditional Probability --> P(X |Y) --> means probablity of X when Y has occured before X. Hence, X is dependent on Y.

recall, example of box A and box B with few points in Red, and black color.

joint probability --> P(X, Y) = P(Y, X) --> probability of events in X and events in Y are independent of each other.

Bayes Theoram --> explains the process and effect of swaping events (Y) and result (X) in conditional propability.
      means how P(X | Y) can be converted into P(Y | X)


Discriminant analysis --> is a statistical analysis technique which classifies based on hypothesizing the per class conditional probability distribution to be normal and pinning down these parameters by data fitting.

Probability distribution is mostly used for quantitaive measures, means for regression problems. 

But for classification problems, probability measure is used.

Bayes classifier minimizes the probability of mis-classification.

For unbalanced classes, the classical Bayesian decision theory allows us to
handle the scenario when the minority class is of particular interest.

Topics that I need to understand properly -->

1. Discriminant Analysis
2. Linear DA
3. Quadratic DA
4. Naive Bayes
5. Gaussian Naive Bayes
6. MultinomialNB
7. BernoulliNB

Discriminant Analysis -->

1. To build a Bayes classifier, we need to understand probability distribution of each class. Since this is a continuous distribution, the Gaussian distribution is widely used to model it. Different assumptions on Gaussian distribution result in different kind of classifiers. The following three are most common:

Linear Discriminant Analysis (LDA)
Quadratic Discriminant Analysis (QDA)
Gaussian Naive Bayes (This is the same as QDA in a one dimensional case)

Bayes Theorem --> It describes the probability of an event, based on some prior condition that might be related to the event.

Discriminant Analysis: Basic Theory
      When we label the data set according to the classification outcome, we may hypothesize that each sub-data set is generated from certain type of probability distributions. Discriminant analysis is a statistical technique which classifies by determining the probability distributions of all classes based on the data.

The type of probability distribution is hypothesized. But the specific distribution is pinned down by data fitting

Bayes Theorem --> Explains how conditional probability P(X | Y) relates to P(Y | X). This will be helpful to solve many classification problems. Because in many scenario we know probability of one vs other and this is the way we can relate known probability with unknown.

Bayes theorem comes into play because we want to relate the two conditional probabilities. 

Different models (hypotheses on the probability distributions) result in different types of classifiers.

Bayes Classifier --> Once we can predict the probability of an observation belonging to each class category, we can then classify the observation to the class with the highest estimated probability. It minimises the probability of misclassification.

The boundary of classification (decision boundary) is simply the location where the probabilities of different classes become the same.

To understand probability of any class its important to understand the prior probability of each class in conditional probability. In discriminant analysis this becomes important while size of two class is different.

Discriminant Analysis: Modeling --> from here onwards all points in ipython notebook are good. you can read from there itself.





