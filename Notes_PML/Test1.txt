Lecture 1 -->

ML wouldn't directly land you to job, you need good coding and Data Analysis skills to thrive in job.
Once you get those skills then your ML skills will help you.

If you feel difficulty in learning some lectures, you should master atleast below 4 topics -->

1) Linear Regression
2) Multiple Linear Regression
3) Logistic Regression
4) Discriminant Analysis
5) Model Selection (Cross Validation)

Always become Coherent in the skills that you want to excel in your life.

What is ML --> ML try to understand and learn from the data/examples/observations and no need to write the program. And its much more efficient than hand-written programmed. Hence, ML can be written as "Programming by Example"

Hence, to understand the data pattern we need to understand the statistics part of it.
On the other side, these statistical computation are very complex and hence we need computer capabilities to solve it faster. Hence, computer Science comes into picture of ML to make these ML algorithm more optimized, performance based result.
These are the 2 major components of ML.

Hence, RML is more statistical version on ML
while PML is more towards Computer Science background of ML.

Example:-  Tree Recognition in picture, Amazon product Recommendation system. (These would be a good example for your capstone project)

2 Main type of ML --> 
Supervised Learning --> 
  Inferential (focus more on effect of one variable on another) and Predictive Tasks (focus more on high accuracy of output variable) 
  predict outcome based on input variables
  Understand the relationship between variables
  Regression Problem, Y is quantitative (eg, price), predict continous outcome
  Classification Problem, Y is categorical (eg, digits 0-9), predict discrete output (example, yes or no)

Unsupervised Learning -->
  No outcome variables, input is often called features, Outcome is more fuzzy, 
  example, cluster the output in segments or seperate groups

Scikit-Learn is the majorly used package for PML. Because this one package provides almost all major ML algorithms. Only few exceptions.
https://scikit-learn.org

Remember, to use as much picture as possible in your project.
Remember, its not important today what project you did, but the skills that you learned through these projects is what matters the most.

Remember, to allocate your time in project some percent in coding, in data analysis, and then in ML concepts.

Simple Linear Regression --> Uncover the relationship between continous variable

  Y = beta0 + beta1 *  X + epselon
  here, 
    beta0 --> intercept of the regression line
    beta1 --> slope of regression line
    epselon --> this is the error or residual term, this indicates that for a given value of x if we need ideal Y value then we need to add this error term to make it on regression line.
 
Difference between mathematician and Statistitian --> Mathematician says I need a formula to understand and solve a problem, but statistitian says if there is no concrete formula we can add some error terms to need and analyze the result or impact of it.

Simple Linear Regression and Multiple LR are the two most coomonly known model that makes the good amount of assumptions to make the model work accurately. But in advanced supervised learning models like Support vector machine, random forest, etc this is not the case. Those models doesn't have so many assumptions.

Below are 4 major assumptions of Linear Regression Model:
1. Linearity --> This means, X and Y has linear relationship. If they are non-linear the model doesn't work accurately. This means X and Y observations can be represented by Linear regression formula. This assumption is completely for X and Y observations.

on the other side, below 3 assumptions are for residuals or errors. As we can't estimate epselon (error) because it is random, hence we can study the properties of randomness.

Randomess mostly described by probability distribution / nomality distribution / Gaussian distribution.

2. Normality --> This assumption consider that all errors comes from same Gaussian distribution or normal distribution.
Need to understand, what is Gaussian distribution.
Normality of erros, can also be understood by ploting a histogram of all errors for a linear regression line. This histogram will give un-normalized distribution. We can calculate the mean and standard deviation of sample errors. Then we will plot a pdf curve (probablity density function) on this histogram. 

Then we create a Gaussian distribution for the same mean and standard deviation. if both are representing same curve then we can assume that the errors are normaly distributed.

if there is fate tail or if the curve or histogram doesn't show normality then Linear model doesn't work accurately.

Apart from that, below two assumptions Constant Variance and Independent Errors, indicates that error of each observation follow the same distribution.
However, there is no good way to plot a distribution for one observation. Hence, we consider to plot a distribution for subset of dataset and observe that if we take full dataset vs subset of dataset, are they both showing nearly same distribution.

3. Constant Variance --> all histogram of individual error shows same variance.
In simple words, variance measures how far a set of (random) numbers are spread out from their average value.

4. Independent Errors --> Each error in regression is independent of one another. Means, size and direction of error is completely independent of one another. Also, refered to as IID (Identitical independent variables)

Violating any of these assumptions will lead to inaccurate model. Need to understand how these assumptions impact the model.

If you see in any dataset, your model is showing any discripencies in any of these assumptions you can either change the model or change the feature and understand the impact of it on model.

   Estimating Coefficient --> 
          In real world, in general, slope and intercept are unknown for a linear model, we just know X and Y observations. Hence, the question is how to determine an optimal slope and intercept. Hence, which line would be a better fit for this kind of pattern and model.

Inorder to understand the best fit line, we calculate residual for various coefficients and try to minimize the error/residule. The best line will be with least error.

e = Y - beta0 - beta1 * X
e = (e1, e2, e3, e4, ......, en)

To quantify the difference between the model and the observation, Residual Sum of Squares will be calculated. Because, this will be give us an idea about the area that an error can cover.

RSS = e1^2 + e2^2 + e3^2 +..... + en^2
Hence, RSS = sum of (Y - beta0 - beta1 * X)

Why we squared the error, because negative and positive both errors can become the part of RSS. If we just take sum without squares, then negative and positive errors can cancle then out and it  

Hence, RSS completely depends on beta0 and beta1.

If you visualize the RSS, it looks like concave. Hence, RSS plot has concave property. Hence, our goal to find the minimum of RSS. That will tell us the best fit model.

Most of the models support concave property but nural network and few others are the example of model that doesn't support concave property.

Every concave will have only one unique point that will be minimum center of the concave.

RSS and R-square are different entities, R-sqaure always calculated in between 0 and 1. but RSS can be more than 1.
Although, you can say they are relate to each other.

The co-efficient that really minimizes the RSS is denoted by beta0-hat and beta1-hat, which is also known as OLS (Ordinary Least squares) estimator.

The regression line always pass throught the center and which means X-bar, Y-bar, which is center of all x and y points.
  beta0-bar = Y - beta1-bar * X

and slope co-efficient is the ratio of (covariance of X, Y) and (variance of X).
Hence, The slope is controlled by the co-relation of input and output. That is X and Y.

How to measure the accuracy of Co-efficient --> 
  Calculating Standard Error is the approach that we can use to understand the accuracy of our model. This SE will tell us how far our model is from perfect model. Also, mathematically its proved that as we increase the number of observation, SE for slope gets smaller.
  
  This concept is used in Variance biased trade-off. we will learn in later lecture.
  Hence, this SE estimates the plausible range in which true regression lies within.
  
  The sum of SE^2 (SE-square), known as model variance of simple linear model. This tells us the overall range of model for the chosen sub-dataset. The tight model variance means a good model, while a wide range of SE indicates model is not trustworthy.
  
  Similar to this we can calculate the Standard Error for our residuals/errors. This is also known as Residual Standard error. 

V. imp --> Once we calculate the standard error, now we can asses the accuracy of model coefficient by -->
--> performing hypothesis test
--> constructing confidence interval

Major drawback of RSS, RSS doesn't give us any concrete idea about model for different set of observations. Every sub-dataset will bring different RSS value and hence its hard to asses the overall accuarcy of model.
This is calculated by R-square, also known as co-efficient of determination.

   Coefficient of Determination --> This tells us how much of your dataset spread around the linear model and the remaining is residual.
        for example, if R-square is 0.6, then the dataset (X) spread around the model is 60% and 40% is residuals.
          similarly, if R-square is 0.9, then the dataset (X) spread around the model is 90% and 10% is residuals. 
          hence,     if R-square is 1, then dataset completely spread on linear model line, and there is no error.
         
         R-square = 1 - (RSS/TSS)
         Here, TSS --> total sum of square of Y
         
  Question, why RSS is always smaller or equal to TSS, 
         What is R-square, It is how much better your regression line is than a simple horizontal line through the mean of the data. 
         RSS, represents the squared residual with respect to regression line
         TSS, represents the squared residual with respect to average value of Y and its fixed if data is known.


